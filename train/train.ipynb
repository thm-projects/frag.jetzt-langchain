{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9517092b-08b8-4646-b213-deb84b3acb71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e70a56e1-7a5c-4df7-89a9-fde7b278316a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_data():\n",
    "    with open(f\"multihatespeech/german-oai-mhd.json\") as f:\n",
    "        oai = json.load(f)\n",
    "    with open(f\"multihatespeech/german-celadon-mhd.json\") as f:\n",
    "        celadon = json.load(f)\n",
    "    with open(f\"multihatespeech/german-detox-mhd.json\") as f:\n",
    "        detox = json.load(f)\n",
    "    return {\"oai\": oai, \"celadon\": celadon, \"detox\": detox}\n",
    "\n",
    "moderation_data = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03ce329d-c0d3-41d4-9b28-944372c43692",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"streamlit-data.json\") as f:\n",
    "    labeled = json.load(f)[\"german\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e01085f-6b51-4b6e-969b-28eb4f771ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75239</th>\n",
       "      <td>Apropos US-Kreigsverbrechen:  Wo bleiben eigen...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75240</th>\n",
       "      <td>Logo - die Airportslots von Ex-Berlin kassiert...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75241</th>\n",
       "      <td>Das hat der Günstling von seiner Kanzlerin gel...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75242</th>\n",
       "      <td>Die `jungen Leute`, die die F1 mit mehr Stadtr...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75243</th>\n",
       "      <td>Vater lässt sein Kleinkind alleine am Ufer zur...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102549</th>\n",
       "      <td>Der Typ sonnte sich doch nur noch in den Erfol...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102550</th>\n",
       "      <td>Bei verkehrsrechtlicher Nötigung anderer Verke...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102551</th>\n",
       "      <td>Vieleicht sollte der KFC mal langsam über eine...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102552</th>\n",
       "      <td>Dann aber auch automatische Waffen für Schüler...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102553</th>\n",
       "      <td>Soweit zum Demokratieverständnis der Grünen.: ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27315 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "75239   Apropos US-Kreigsverbrechen:  Wo bleiben eigen...    0.0\n",
       "75240   Logo - die Airportslots von Ex-Berlin kassiert...    0.0\n",
       "75241   Das hat der Günstling von seiner Kanzlerin gel...    1.0\n",
       "75242   Die `jungen Leute`, die die F1 mit mehr Stadtr...    0.0\n",
       "75243   Vater lässt sein Kleinkind alleine am Ufer zur...    1.0\n",
       "...                                                   ...    ...\n",
       "102549  Der Typ sonnte sich doch nur noch in den Erfol...    1.0\n",
       "102550  Bei verkehrsrechtlicher Nötigung anderer Verke...    0.0\n",
       "102551  Vieleicht sollte der KFC mal langsam über eine...    0.0\n",
       "102552  Dann aber auch automatische Waffen für Schüler...    1.0\n",
       "102553  Soweit zum Demokratieverständnis der Grünen.: ...    1.0\n",
       "\n",
       "[27315 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\n",
    "    \"./MultiLanguageTrainDataset.csv\", usecols=[\"text\", \"label\", \"language\"]\n",
    ")\n",
    "data = data[data.language == 5]\n",
    "data = data.drop(columns=[\"language\"])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "113038cd-b910-4532-9ad2-596eced300f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL = \"distilbert/distilbert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "labels = [\n",
    "    \"outside\",\n",
    "    # Content that incites or glorifies physical harm or aggression, including threats.\n",
    "    \"violent\",  # Example: \"I'm going to hurt you, and you deserve it.\"\n",
    "    # Content that is vulgar, explicit, or offensive in language or sexual nature.\n",
    "    \"obscene\",  # Example: \"What the **** is wrong with you, you piece of ****?\"\n",
    "    # Content that includes persistent unwanted behavior or personal attacks.\n",
    "    \"harassment\",  # Example: \"You're a failure, and everyone knows it.\"\n",
    "    # Content that demeans, attacks, or excludes based on personal or group attributes.\n",
    "    \"hate_discrimination\",  # Example: \"People like you shouldn't exist.\"\n",
    "    # Content that promotes self-harm, suicide, or glorifies injury.\n",
    "    \"self_harm\",  # Example: \"Cutting yourself is the only way to feel better.\"\n",
    "    # Content that is contextually inappropriate or violates the norms of a specific audience.\n",
    "    \"inappropriate\",  # Example: Sharing adult-themed jokes in a children's forum.\n",
    "]\n",
    "\n",
    "label_to_id = {k: i for i, k in enumerate(labels)}\n",
    "id_to_label = {i: k for i, k in enumerate(labels)}\n",
    "    \n",
    "\n",
    "def is_not_flagged(data, i, source):\n",
    "    if source.iloc[i][\"label\"] == 1.0:\n",
    "        return False\n",
    "\n",
    "    for key, v in data[\"oai\"][i].items():\n",
    "        if type(v) is not float:\n",
    "            if key == \"flagged\" and v:\n",
    "                return False\n",
    "            continue\n",
    "        if v > 0.4:\n",
    "            return False\n",
    "\n",
    "    for key, v in data[\"celadon\"][i].items():\n",
    "        if type(v) is not float:\n",
    "            if key == \"Flagged\" and v != \"No\":\n",
    "                return False\n",
    "            continue\n",
    "        if v > 0.7:\n",
    "            return False\n",
    "\n",
    "    for key, v in data[\"detox\"][i].items():\n",
    "        if type(v) is not float:\n",
    "            continue\n",
    "        if v > 0.25:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def add_to_labels(v, labels):\n",
    "    ranges = []\n",
    "    # collapse ranges!\n",
    "    for elem in labels:\n",
    "        _, start, end, label = elem\n",
    "        label = label_to_id[label]\n",
    "        last_found = len(ranges)\n",
    "        for i, r in enumerate(ranges):\n",
    "            if end < r[0]:\n",
    "                last_found = i\n",
    "                break\n",
    "            if start > r[1] or r[2] != label:\n",
    "                continue\n",
    "            ranges[i] = [min(start, r[0]), max(end, r[1]), r[2]]\n",
    "            last_found = -1\n",
    "            break\n",
    "        if last_found != -1:\n",
    "            ranges.insert(last_found, [start, end, label])\n",
    "    # apply to current tokens\n",
    "    for i, k in enumerate(v['offset_mapping']):\n",
    "        if k[0] == 0 and k[1] == 0:\n",
    "            continue\n",
    "        for r in ranges:\n",
    "            if k[1] < r[0]:\n",
    "                break\n",
    "            if k[0] > r[1]:\n",
    "                continue\n",
    "            v['labels'][i] = r[2]\n",
    "    del v['offset_mapping']\n",
    "\n",
    "def make_complete_dataset():\n",
    "    new_data = []\n",
    "    new_test_data = []\n",
    "    count_p = 0\n",
    "    count_n = 0\n",
    "    for i in range(len(data)):\n",
    "        idx = data.index[i]\n",
    "        text = data.iloc[i][\"text\"]\n",
    "        if str(idx) in labeled:\n",
    "            v = tokenizer(text, return_offsets_mapping=True)\n",
    "            v['labels'] = [0] * len(v['input_ids'])\n",
    "            add_to_labels(v, labeled[str(idx)])\n",
    "            if count_p < 1:\n",
    "                count_p += 1\n",
    "                new_test_data.append((i, v))\n",
    "            else:\n",
    "                new_data.append((i, v))\n",
    "        elif is_not_flagged(moderation_data, i, data):\n",
    "            v = tokenizer(text)\n",
    "            v['labels'] = [0] * len(v['input_ids'])\n",
    "            if count_n < 15:\n",
    "                count_n += 1\n",
    "                new_test_data.append((i, v))\n",
    "            else:\n",
    "                new_data.append((i, v))\n",
    "    return new_test_data, new_data\n",
    "\n",
    "new_test_data, new_data = make_complete_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af3b37f7-3812-4c11-9ef3-c93497515659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index][1]\n",
    "        return x\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "test = MyDataset(new_test_data)\n",
    "train = MyDataset(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21d7f2b1-bf09-42ee-acba-69b8c5b65258",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "label_list = labels\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be375884-70a7-4f6f-b8a2-a1bd5178e42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL, num_labels=len(labels), id2label=id_to_label, label2id=label_to_id\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"distilbert-token-swearword\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c047d49e-7c3c-45c1-8ac5-7950c22a3a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1062' max='1062' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1062/1062 08:52, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.028100</td>\n",
       "      <td>0.075451</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.992521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.072872</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.992521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruben/anaconda3/envs/langchain-training/lib/python3.13/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: outside seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/ruben/anaconda3/envs/langchain-training/lib/python3.13/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: inappropriate seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/ruben/anaconda3/envs/langchain-training/lib/python3.13/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ruben/anaconda3/envs/langchain-training/lib/python3.13/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: outside seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/ruben/anaconda3/envs/langchain-training/lib/python3.13/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: inappropriate seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/ruben/anaconda3/envs/langchain-training/lib/python3.13/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1062, training_loss=0.015994164791035336, metrics={'train_runtime': 533.7606, 'train_samples_per_second': 31.834, 'train_steps_per_second': 1.99, 'total_flos': 501663429461568.0, 'train_loss': 0.015994164791035336, 'epoch': 2.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b941f751-f742-42aa-ba88-cee866da3de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_with_str(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    for key in inputs.keys():\n",
    "        inputs[key] = inputs[key].to('cuda:0')\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    predictions = torch.argmax(logits, dim=2)\n",
    "    return [model.config.id2label[t.item()] for t in predictions[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a9c4ac-08be-4ce0-a85d-62231258a608",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-train",
   "language": "python",
   "name": "conda-train"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
